<!DOCTYPE HTML>
<!--
	Based on the template 'Retrospect', by TEMPLATED
	Template released for free under the Creative Commons Attribution 3.0 license (templated.co/license)

	Content by Luis Vale Silva
-->
<html lang="en">
	<head>
		<title>Data simple - Random Forests</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="description" content="An exploration of word clouds using Andreas
		Mueller's word_cloud Python library.">
		<meta name="keywords" content="data science, data analysis, data visualization,
		python, scikit-learn, machine learning, random forests, decision trees">
		<meta name="author" content="Luis Vale Silva">
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<link rel="icon" href="images/datasimple_favicon.ico" type="image/x-icon">
		 <link rel="stylesheet" href="assets/css/prism.css">
		 <script src="assets/js/prism.js"></script>

		 <!-- Google Analytics -->
		<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71624249-1', 'auto');
  ga('send', 'pageview');
  </script>

        <!-- Mathjax -->
        <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
	</head>
	<body>

	<!-- Header -->
	<header id="header" class="skel-layers-fixed">
		<h1><a href="index.html">Home</a></h1>
	</header>

	<!-- Main -->
	<section id="main" class="wrapper">
		<div class="container">
			<header class="major special">
				<h4><b>May 2017</b></h4>
				<h2>Random Forests</h2>
        <h3>Growing a forest of random Decision Trees with scikit-learn</h3>
			</header>
        <center>
            <p><font size="4" color="black">Full code on
            <font size="6"><a href="https://github.com/luisvalesilva/random_forests" target="_blank" class="icon fa-github"></a></font></font>
            </p>
        </center>
        <br />
        <p><i>Random Forests</i> are a popular example of an <i>Ensemble Learning</i> method. Ensemble Learning consists on combining multiple machine learning (ML) models in order to achieve higher predictive performance than could be obtained using either of the individual models alone. Ensemble Learning methods work best when the models are as diverse as possible, providing complementary predictive performance. One way to achieve this is to combine models built using different ML algorithms, to leverage their complementary strengths. An alternative approach is to use the same ML algorithm, but to train each model on a different random subset of the training data set. This is the appproach taken by Random Forests: they combine multiple <i>Decision Tree</i> models trained on different subsets of the training data.</p>
          
        <p>I will start by taking a closer look at the Decision Tree ML algorithm, which is at the core of Random Forests. But before moving on, let's generate some toy data using <a href="http://scikit-learn.org/stable/" target="_blank">scikit-learn</a>'s <code1>make_blobs</code1> function. Let's make it a two-dimensional data set, in order to allow easy visualization. There will be, say, 300 observations evenly split between three classes, "Yellow", "Blue", and "Red". I'll use a high-enough standard deviation (parameter <code1>cluster_std</code1>) to blur out the class boundaries a little bit.</p>

        <pre>
            <code class="language-python">
import matplotlib.pyplot as plt

# Generate data
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=300, n_features=2, centers=3,
                  cluster_std=4, random_state=42)

# Plot data
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", marker='.')
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", marker='.')
plt.plot(X[:, 0][y==2], X[:, 1][y==2], "rd", marker='.')
plt.xlabel(r"$x_1$", fontsize=20)
plt.ylabel(r"$x_2$", fontsize=20, rotation=0)
plt.title("Toy data set\n", fontsize=16)</code>
        </pre></br>

    	<!-- Image -->
        <center>
            <div class="8u"><span class="image fit"><img src="images/random_forests/toy_data.png" alt="Toy data" /></span></div>
		</center>

        <header>
            <h3>1. Decision Trees</h3>
            <h4>1.1 Making predictions</h4>
        </header>
        <p>Decision Trees are powerful ML algorithms capable of fitting complex data sets. The Wikipedia page on <a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank">Decision Tree Learning</a> presents the following definition:</p>
        
        <blockquote>A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node.</blockquote>

        <p>In order to classify a data instance, all you need to do is traverse the flow chart, answering all the tests, until you reach a leaf node. Each test will be a question conerning a specific feature in the data. The class labeling of the leaf node the new instance ends up in is the predicted class. Let's grow a Decision Tree on our data to take a closer look at how this works in practice. scikit-learn makes this ridiculously easy, of course.</p>

        <pre>
            <code class="language-python">
from sklearn.tree import DecisionTreeClassifier

# Instantiate the classifier class
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)

# Grow a Decision Tree
tree_clf.fit(X, y)</code>
        </pre></br>

        <p>We have grown a tree on our 2-D data set. Classifying new examples is simple. Let's make up a couple of examples to try the classifier.</p>

        <pre>
            <code class="language-python">
import numpy as np

def predict_class(data_point, classes=['Yellow', 'Blue', 'Red']):
    # Convert to appropriate numpy array
    data_point = np.array(data_point).reshape(1, -1)
    
    # Classify
    result = tree_clf.predict(data_point)
    
    # Print output
    print('Predicted class for point {}: {}'.format(data_point,
                                                    classes[np.asscalar(result)]))
    
predict_class([-5, 10])
predict_class([10, 5])
predict_class([-10, -10])</code>
        </pre>

        <pre><code>
Predicted class for point [[-5 10]]: Yellow
Predicted class for point [[10  5]]: Blue
Predicted class for point [[-10 -10]]: Red
</code></pre><br />
 
        <p>If you check the location of the points visually in the plot above you will see that these predictions make sense. We can visualize the Decision Tree model to inspect its structure and see how it makes decisions. We can use the <code1>dot</code1> tool in the <a href="http://www.graphviz.org/" target="_blank">graphviz</a> package. We start by generating a graph definition as a <code1>.dot</code1> file, using the <code1>export_graphviz</code1> method.</p>

        <pre>
            <code class="language-python">
from sklearn.tree import export_graphviz

export_graphviz(tree_clf, out_file='tree.dot',
                feature_names=['x1', 'x2'],
                class_names=['Yellow', 'Blue', 'Red'],
                rounded=True, filled=True)</code>
        </pre>

        <p>We can then convert the <code1>.dot</code1> file to a <code1>.png</code1> image file and visualize the tree structure.</p>
 
        <pre>
        <code class="language-bash">
dot -Tpng tree.dot -o tree.png</code>
        </pre></br>

    	<!-- Image -->
        <center>
            <div class="8u"><span class="image fit"><img src="images/random_forests/tree.png" alt="Tree" /></span></div>
		</center>
        
        <p>We can now see the exact way the model predicts new instance classes. At the root node we find all 300 training examples split evenly between the three classes. The tree asks whether the value of feature \(x2\) is lower than or equal to \(−1.3707\). It splits the data in two groups according to the answer and then goes on to ask another question in each of the two following internal nodes. These, in turn, split the data between two groups each. Since we set argument <code1>max_depth=2</code1>, the tree stops here, with four leaf nodes providing the final prediction.</p>

        <p>Let's use one of the instances predicted above as an example: \(x1=−5, x2=10\). The answer to the root node question is <code1>False</code1>, since \(x2 > −1.3707\). The answer to the next question is <code1>True</code1>, since \(x1 <= 0.8566\). This brings us to the leaf node predicting class "Yellow".</p>

        <p>This structure allows Decision Trees to estimate class probabilities by simply computing the ratio of training instances belonging to each class in the leaf node. For example, for our predicted instance above this tree estimates it belongs to class "Yellow" with 84.6% probability (88/104; the predicted class), class "Blue" with 8.7% probability (9/104) and class "Red" with 6.7% probability (7/104).</p>

        <p>These rules define axis-parallel decision boundaries, dividing the feature space into a series rectangles, each of which predicts only one of the classes. Let's define a function to visualize the decision boundaries and a function to plot them, together with the training data, using matplotlib.</p>


        <pre>
            <code class="language-python">
from matplotlib.colors import ListedColormap

def compute_decision_boundaries(clf, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 300)
    x2s = np.linspace(axes[2], axes[3], 300)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    
    return x1, x2, y_pred

def plot_feature_space(clf, X, y, axes, file_name=None):
    x1, x2, y_pred = compute_decision_boundaries(clf, X, y, axes)
    custom_cmap = ListedColormap(['y','b','r']) plt.contourf(x1, x2, y_pred, cmap=custom_cmap, alpha=0.1, linewidth=1) plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", marker='.') plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", marker='.') plt.plot(X[:, 0][y==2], X[:, 1][y==2], "rd", marker='.')
    plt.axis(axes)
    plt.xlabel(r"$x_1$", fontsize=20)
    plt.ylabel(r"$x_2$", fontsize=20, rotation=0)
    if file_name is not None:
        plt.savefig(file_name, dpi=300, bbox_inches='tight')
    plt.figure(figsize=(8, 4))</code>
        </pre>

        <p>Now we can plot the decison space for this example.</p>
        
        <pre>
            <code class="language-python">
plot_feature_space(tree_clf, X, y, axes=[-16, 15, -20, 20], file_name='max_depth2.png')</code>
        </pre>

    	<!-- Image -->
        <center>
            <div class="8u"><span class="image fit"><img src="images/random_forests/max_depth2.png" alt="Max depth 2" /></span></div>
		</center>

        <header>
            <h4>1.2 "Growing" a Decision Tree model</h4>
        </header>

        <p>So this is how a Decision Tree model makes predictions. But how can we actually grow the tree? There are alternatives, but scikit-learn uses the <i>Classification And Regression Tree</i> (CART) algorithm. CART works by splitting the training set in two subsets using a single feature \(k\) and a threshold \(t_k\), such that the split produces the purest subsets (weighted by their size). This is called the <i>splitting rule</i>. It then splits the subsets in the same exact way, then the sub-subsets and so on, recursively, until it cannot find a split to reduce class impurity. This is known as contructing the <i>maximum tree</i>. The CART algorithm minimizes the following cost function at each split:</p>

        $$ J(k, t_k) = \dfrac{m_{left}}{m}I_{left} + \dfrac{m_{right}}{m}I_{right} $$

        $$
        with
        \begin{cases}
            I_{left/right} \text{ impurity of the left/right subset,}\\
            m_{left/right} \text{ number of instances in the left/right subset.}
        \end{cases}
        $$

        <p>So what is the impurity \(I\) measure? Typically, this will be the <i>Gini impurity</i> measure. Formally:</p>

        $$ I_G(t) =  1 - \sum_{k=1}^K p_{k,t}^2 $$

        $$
        with
        \begin{cases}
            k \bbox[3pt]{1, . . . , K} \text{ index of the class,}\\
            p_{k,t} \text{ ratio of class $k$ instances among the training instances in node $t$}
        \end{cases}
        $$

        <p>In other words, the Gini impurity measures the probability that a randomly chosen instance from the node would be incorrectly labeled if it was randomly labeled according to the distribution of classes in the node. It does this by first computing the probability \(p_{k,t}\) that a randomly chosen instance from the subset of training instances in the node \(t\) belongs to class \(k\) (simply the ratio of class \(k\) instances). It multiplies this probability by itself, which yields the probability \(p_{k,t}^2\) that that instance would be correctly labeled if it was randomly labeled according to the distribution of classes in the node (again, simply the ratio of class \(k\) instances). It then sums up the probabilities across all classes \(K\) and finally computes the probability that the instance would be incorrectly classified by subtracting the previous probability from the unity. The Gini impurity is thus reduced to zero when the node is pure, i.e. all instances in the node belong to the same class, since in that case \(p_{k,t} = 1\) for the represented class (and 0 for all others) and thus \(p_{k,t}^2 = 1\), resulting in \(I_G(t) = 1 - 1 = 0\).</p>

        <p>You can now go back to the image of the tree structure and check the "gini" attribute at each node. Let's take, for example, the purple leaf node \(t=4\). Using the definition above, we can calculate the Gini impurity at this is node and get the indicated value of \(0.0425\):</p>

        $$ I_G(2) =  1 - \sum_{k=1}^K p_{k,2}^2 = \\
        = 1 - \left( {\left(\dfrac{0}{92}\right)}^2 + {\left(\dfrac{2}{92}\right)}^2 + {\left(\dfrac{90}{92}\right)}^2 \right) =\\
        = 1 - ( 0 + 0.00047 + 0.95699 ) =\\
        = 0.0425 $$

        <p>This is quite a pure node, since 97.8% of the observations belong to the same class ("Red"). Its Gini impurity measure is consequently very low. An alternative is to use the <i>Information gain</i> measure, based on the concept of <i>entropy</i> from information theory. In scikit-learn you can use this measure by changing <code1>criterion='entropy'</code1> instead of <code1>'gini'</code1> when you instantiate the <code1>DecisionTreeClassifier</code1>.</p>

        <p>Decision Trees have the ability to adapt very closely to the training data. They are <i>non-parametric models</i>, since the number of parameters is not predetermined and will thus grow during traning. This means that they have a tendency to produce high-variance models that will not generalize well to classification of new, previously unseen, instances. This is known as overfitting the training data. For this reason, regularization is very important when training Decision Tree models. Decision Trees have several regularization hyperparameters. Different implementations will, at the very least, allow you to constrain the tree depth. You may have noticed that I have used that above, by instantiating the <code1>DecisionTreeClassifier</code1> with argument <code1>max_depth=2</code1>. This was mainly to generate a small tree that was easy to visualize. The result was arguably an underfitting model that could be improved. Here is what it looks like using <code1>max_depth=3</code1>.</p>

        </div>
  </section>
  <!-- Footer -->
  <footer id="footer">
    <div class="inner">
      <font size="1">
        <ul class="copyright">
          <li>&copy; Data simple</li>
          <li></li><li></li>
	  <li>Hosted on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.</li>
          <li></li><li></li>
          <li>Design based on 'Retrospect' from <a href="http://templated.co" target="_blank" >TEMPLATED</a>.</li>
        </ul></font>
      </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
    <script src="assets/js/main.js"></script>

		<!-- GetSiteControl.com widgets -->
		<script>
		(function (w,i,d,g,e,t,s) {w[d] = w[d]||[];t= i.createElement(g);
			t.async=1;t.src=e;s=i.getElementsByTagName(g)[0];s.parentNode.insertBefore(t, s);
		})(window, document, '_gscq','script','//widgets.getsitecontrol.com/49202/script.js');
		</script>
  </body>
</html>
